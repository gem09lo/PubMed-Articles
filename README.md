# PubMed Articles ETL Project

## ğŸ§  Overview

This project processes scientific publication data from PubMed in XML format, aiming to extract author metadata, match affiliation names to institutional GRID identifiers using NLP and fuzzy matching, and store the final structured data for analysis.

The goal was to automate and streamline the process of extracting, cleaning, and enriching article metadata, as well as to build deployable infrastructure that allows for scalable and maintainable data ingestion using AWS services.

This project was completed over one week and includes both local development capabilities and production deployment via AWS ECS and Terraform.

---


### ğŸ“Œ Stakeholder

**Eve Chen (Director of Immunology)**

This project was developed for Eve, a health data scientist working in the research team at Pharmacer. Eve is responsible for analyzing research articles to extract information on authors, their institutions, and associated metadata to support research collaboration and pharmaceutical development. 

The goal of this pipeline is to automate what was previously a time-consuming manual task: matching institution names and extracting structured metadata from PubMed XML files, enabling more efficient and accurate data analysis for downstream work.

---


## ğŸ¯ Project Aims

- Parse large-scale PubMed XML data and extract key metadata (e.g. article titles, authors, affiliations).
- Use NLP and fuzzy matching to identify author institutions and link them to GRID IDs.
- Generate clean CSV outputs suitable for further analysis and dashboarding.
- Automatically notify stakeholders of ETL pipeline status via AWS SES email.
- Deploy the pipeline as a scheduled or event-driven ECS task using Terraform.

---

## ğŸ“ Project Structure

pubmed-articles/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ pipeline/ # Core ETL logic (extract, transform, load, notify)
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â”œâ”€â”€ load.py
â”‚ â”œâ”€â”€ etl.py
â”‚ â”œâ”€â”€ trigger.py
â”‚ â””â”€â”€ dockerfile
â”‚
â”œâ”€â”€ raw_data/ # Raw XML files and institutional reference data
â”‚ â”œâ”€â”€ c14-gem-lo-pubmed.xml # Sampled dataset
â”‚ â”œâ”€â”€ pubmed_result_sjogren.xml # Full dataset
â”‚ â”œâ”€â”€ pubmed_result_start.xml # Initial raw file (used in trigger.py)
â”‚ â”œâ”€â”€ institutes.csv # GRID institution list
â”‚ â”œâ”€â”€ aliases.csv # [Optional] GRID alias data
â”‚ â”œâ”€â”€ addresses.csv # [Optional] GRID address data
â”‚
â”œâ”€â”€ cleaned_data/ # Output CSVs from the transform stage
â”‚ â”œâ”€â”€ pubmed_output.csv # Processed sample output
â”‚ â”œâ”€â”€ pubmed_output2.csv # Full processed dataset
â”‚ â”œâ”€â”€ matched_sample.csv
â”‚ â””â”€â”€ unmatched_sample.csv
â”‚
â”œâ”€â”€ data_analysis/ # Jupyter notebooks for exploration and testing
â”‚ â”œâ”€â”€ pubmed_result.ipynb # Full dataset processing
â”‚ â””â”€â”€ pubmed_test.ipynb # Small dataset for test/debugging
â”‚
â”œâ”€â”€ terraform/ # AWS Infrastructure-as-Code
â”‚ â”œâ”€â”€ main.tf
â”‚ â”œâ”€â”€ variables.tf
â”‚ â”œâ”€â”€ terraform.tfvars (gitignored)
â”‚ â””â”€â”€ README.md # Setup and deployment instructions


---

### âš™ï¸ Technologies Used
**Tool / Service	Role**
`Python`	:ETL pipeline implementation
`spaCy`	:Named Entity Recognition (NER)
`RapidFuzz`	:String similarity matching
`pandarallel`	:Multiprocessing support for large DataFrames
`AWS S3`	:Input/output data storage
`AWS ECS Fargate`	:Serverless task execution
`AWS SES`	:Email notifications
`Terraform`	:Infrastructure provisioning
`Jupyter Notebooks`	:Development and data validation

## ğŸ”§ Setup Instructions

### 1. Clone the Repository

- `git clone https://github.com/<your-username>/pubmed-articles.git`
- `cd pubmed-articles`


### 2. Create a Virtual Environment and Install Requirements

- `python -m venv .venv`
- `source .venv/bin/activate`
- `pip install -r requirements.txt`

### 3. Download spaCy Language Model

- `python -m spacy download en_core_web_sm`


### ğŸš€ Run ETL Pipeline Locally (No AWS)

To run the ETL pipeline locally on a smaller dataset (c14-gem-lo-pubmed.xml) for testing and development:
- `cd pipeline`
- `python etl_dev.py`

This will:
- Load raw_data/c14-gem-lo-pubmed.xml
- Process and match institutions
- Save output to cleaned_data/pubmed_output.csv
- Create matched_sample.csv and unmatched_sample.csv

### â˜ï¸ Deploying to AWS (Terraform)

This project supports automatic deployment to AWS ECS via Terraform. You can:

- Upload raw XML to S3
- Trigger the ECS task via an EventBridge rule
- Receive email notifications on pipeline success/failure

For full setup instructions, see:
â¡ï¸ terraform/README.md


### ğŸ““ Notebooks
Notebooks are located in the data_analysis/ folder:

Notebook:
`pubmed_result.ipynb`: Full dataset, structured analysis
`pubmed_test.ipynb`: Sample data, faster iteration/dev

Both perform similar NLP + fuzzy matching, but test runs are quicker and better for debugging.